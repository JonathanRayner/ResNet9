# title shouldn't appear in toc
* ResNet9 in Tensorflow :noexport:

Implements the 9 layer ResNet of https://github.com/davidcpage/cifar10-fast in Tensorflow (their implementation uses PyTorch), for use in the CIFAR-10 classification task.

This architecture is interesting, because it demonstrates that one can classify CIFAR-10 images at reasonably high levels of accuracy, while using extremely short training times and a single GPU. With careful choice of architecture, hyperparameters, data augmentation, and minimal other tricks, they achieve 94% accuracy with 79s training time on a single Tesla V100. Using more advanced techniques, they get this time down to 26s. See [[https://myrtle.ai/how-to-train-your-resnet/][this series of blog posts]], which describes the process. See also the [[https://dawn.cs.stanford.edu/benchmark/index.html#cifar10-train-time][Stanford DAWNBench]] competition.

Initially, this project just aims to reimplement the model in Tensorflow. Timing and optimization may follow. 

* Contents :TOC:

* Architecture, Hyperparameters

** Our model

We use their architecture and hyperparameters, unless noted otherwise. The architecture is displayed in the following diagram from their Github

[[net_diagram.svg]] 

To be explicit:

1. Convolution Layer (64 filters), Batch Norm, ReLU
2. Convolution Layer (128 filters), Batch Norm, ReLU
3. Max Pooling Layer (factor of 2)
4. Convolution Layer (128 filters), Batch Norm, ReLU
5. Convolution Layer (128 filters), Batch Norm, ReLU
6. Residual Connection (3+5)
7. Convolution Layer (256 filters), Batch Norm, ReLU
8. Max Pooling Layer (factor of 2)
9. Convolution Layer (512 filters), Batch Norm, ReLU
10. Pooling Layer (factor of 2)
11. Convolution Layer (512 filters), Batch Norm, ReLU
12. Convolution Layer (512 filters), Batch Norm, ReLU
13. Residual Connection (10+12)
14. Max Pooling Layer (factor of 4)
15. Flatten
16. Fully Connected Layer (10 classes)
17. Multiplication Layer (multiply all logits by 0.125)
    
Some things to note:

- Batch Norm is performed before every ReLU
- All convolutions are 3x3 filters with stride 1, padding 1. This is implemented by using ~padding='SAME'~ in Tensorflow. All convolution layers and the final fully connected layer have bias disabled.
- We use 

 
** Discussion of their model

- Their choice of optimizer is SGD with ~momentum=0.9~, using [[https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc][nesterov update]]. I'm actually [[https://github.com/davidcpage/cifar10-fast/issues/11][somewhat confused]] about the way they wrote their nesterov update. They include ~weight_decay=5e-5*batch_size~.
- Their model uses the default PyTorch ~epsilon=1e-5~ and ~momentum=0.1~ for Batch Norm layers, as do we. This differs from the Tensorflow defaults ~epsilon=1e-3~ and ~momentum=0.99~. Note that ~momentum=0.1~ in PyTorch is ~momentum=0.9~ in Tensorflow.
- Their model uses the default PyTorch initialization for the convolution and fully connected layers' weights, [[https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L62][which is]] Kaiming uniform. In Tensorflow this is ~he_uniform~.
- The final multiplication layer is explained by the following [[https://myrtle.ai/how-to-train-your-resnet-4-architecture/][quote]]:

#+BEGIN_QUOTE
By default in PyTorch (0.4), initial batch norm scales are chosen uniformly at random from the interval [0,1]. Channels which are initialised near zero could be wasted so we replace this with a constant initialisation at 1. This leads to a larger signal through the network and to compensate we introduce an overall constant multiplicative rescaling of the final classifier. A rough manual optimisation of this extra hyperparameter suggest that 0.125 is a reasonable value. (The low value makes predictions less certain and appears to ease optimisation.)
#+END_QUOTE

In Tensorflow, the default Batch Norm behavior is to initialize the weights with ones and the bias with zeros, so we make no modification to the defaults, but do include the final scalar multiplication layer.

* Preprocessing, Data Augmentation



* Results

* Discussion

- I'm still somewhat confused about why I couldn't reproduce their exact results using the same hyperparameters in Tensorflow - there must be something that I missed. Possible culprits: 
  - using built-in random cutout vs. their implementation, or perhaps something different
  - test

- These speed test contests do encourage "unnatural" levels of fine tuning, leading to brittle models. Making very slight modifications to my parameters (slightly more or less momentum on batch norm, a few pixels more or less padding, a few more or less pixels cutout, etc.) usually degraded the test accuracy noticeably. The question is whether this is just on the margin, or if we are learning nothing from dedicating time to these techniques. One thing that gives us confidence in this model: simply training a few more epochs (say 5, 10, or 20) and shifting the peak learning rate back (say to epoch 8 or 15) easily gives us a few more percent on test accuracy. So by really pushing the boundaries, we can discover some useful tricks and general themes, then just train a little longer and get less brittle results.

* TODO TODO 

Confirm that momentum=0.9 in their model's SGD is momentum=0.9 in our model.

